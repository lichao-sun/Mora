{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from diffusers import StableVideoDiffusionPipeline\n",
        "from diffusers.utils import load_image, export_to_video\n",
        "\n",
        "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "  \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n",
        ")\n",
        "pipe.enable_model_cpu_offload()"
      ],
      "metadata": {
        "id": "3HCqnywJpRus"
      },
      "id": "3HCqnywJpRus",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e745a41-28a8-4bf5-b414-ea2f5b2ad6b4",
      "metadata": {
        "id": "6e745a41-28a8-4bf5-b414-ea2f5b2ad6b4"
      },
      "outputs": [],
      "source": [
        "from diffusers import DiffusionPipeline\n",
        "import torch\n",
        "\n",
        "# load both base & refiner\n",
        "base = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
        ")\n",
        "base.to(\"cuda\")\n",
        "refiner = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
        "    text_encoder_2=base.text_encoder_2,\n",
        "    vae=base.vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        ")\n",
        "refiner.to(\"cuda\")\n",
        "\n",
        "# Define how many steps and what % of steps to be run on each experts (80/20) here\n",
        "n_steps = 40\n",
        "high_noise_frac = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08dd8de9-5578-4541-9171-48b65ba6cfe9",
      "metadata": {
        "id": "08dd8de9-5578-4541-9171-48b65ba6cfe9"
      },
      "outputs": [],
      "source": [
        "def Get_image(prompt):\n",
        "    # run both experts\n",
        "\n",
        "    image = base(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=n_steps,\n",
        "        denoising_end=high_noise_frac,\n",
        "        height=576,\n",
        "        width=1024,\n",
        "        output_type=\"latent\",\n",
        "    ).images\n",
        "    image = refiner(\n",
        "        prompt=prompt,\n",
        "        num_inference_steps=n_steps,\n",
        "        denoising_start=high_noise_frac,\n",
        "        height=576,\n",
        "        width=1024,\n",
        "        image=image,\n",
        "    ).images[0]\n",
        "    image.save('image.png')\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5331d92-1a7c-4aab-9f32-6a6bddc4eca5",
      "metadata": {
        "id": "f5331d92-1a7c-4aab-9f32-6a6bddc4eca5"
      },
      "outputs": [],
      "source": [
        "def Get_Last_Frame(video_path, output_image_path):\n",
        "  # Load the video file using VideoFileClip\n",
        "  from moviepy.editor import VideoFileClip\n",
        "  with VideoFileClip(video_path) as video:\n",
        "      # Get the last frame by going to the last second of the video\n",
        "      last_frame = video.get_frame(video.duration - 0.01)  # a fraction before the end\n",
        "\n",
        "  # Now, we save the last frame as an image using PIL\n",
        "  from PIL import Image\n",
        "  last_frame_image = Image.fromarray(last_frame)\n",
        "  last_frame_image.save(output_image_path)\n",
        "  return last_frame_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29e47acd-9c1f-44b0-ba83-ff07aa15354d",
      "metadata": {
        "id": "29e47acd-9c1f-44b0-ba83-ff07aa15354d"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Assume `initial_image_path` is the path to your initial image\n",
        "# and `pipe` is your AI frame generation function\n",
        "\n",
        "def generate_and_concatenate_videos(initial_image_path, num_iterations=60):\n",
        "    video_paths = []  # To keep track of all generated video paths\n",
        "    current_image_path = initial_image_path\n",
        "\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        # Generate frames based on the current image\n",
        "        if iteration > 0:\n",
        "            image = current_image_path.resize((1024, 576))\n",
        "        else:\n",
        "            image = Image.open(current_image_path).resize((1024, 576))\n",
        "        seed = int(time.time())\n",
        "        torch.manual_seed(seed)\n",
        "        frames = pipe(image, decode_chunk_size=12, generator=torch.Generator(), motion_bucket_id=127).frames[0]\n",
        "\n",
        "        # Export frames to video and save the path\n",
        "        video_path = f\"video_segment_{iteration}.mp4\"\n",
        "        export_to_video(frames, video_path, fps=25)\n",
        "        video_paths.append(video_path)\n",
        "\n",
        "        # Get the last frame of the current video for the next iterationRGB\n",
        "        current_image_path = Get_Last_Frame(video_path, \"1.png\")\n",
        "\n",
        "    # Load and concatenate all video segments\n",
        "    clips = [VideoFileClip(path) for path in video_paths]\n",
        "    final_clip = concatenate_videoclips(clips)\n",
        "\n",
        "    # Save the final video\n",
        "    final_clip.write_videofile(\"final_output_video.mp4\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Get_image(\"A boy\")\n",
        "generate_and_concatenate_videos(\"image.png\",3)"
      ],
      "metadata": {
        "id": "4-9mgeGKpH22"
      },
      "id": "4-9mgeGKpH22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ff0a745-a843-4b87-b198-308106d8b4e6",
      "metadata": {
        "id": "9ff0a745-a843-4b87-b198-308106d8b4e6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
